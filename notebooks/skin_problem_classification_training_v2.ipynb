{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"../src/assets/image/tensorflow_logo.png\" alt=\"TensorFlow Image\" width=\"50\" align=\"left\"/>\n",
    "</p>\n",
    "\n",
    "# Skin Problem Classification Notebook \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Information\n",
    "\n",
    "**Team ID:** C241-PS385  \n",
    "\n",
    "**Members:**    \n",
    "- Stefanus Bernard Melkisedek - [GitHub Profile](https://github.com/stefansphtr)\n",
    "- Debby Trinita - [GitHub Profile](https://github.com/debbytrinita)\n",
    "- Mhd. Reza Kurniawan Lubis - [GitHub Profile](https://github.com/rezakur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen Development Environment\n",
    "\n",
    "For this project, our team opted to utilize Google Colab as our primary development environment. The decision to use Google Colab was primarily driven by its provision of complimentary access to GPU and TPU resources. These resources significantly expedite the model training process, thereby enhancing our productivity and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import io\n",
    "import itertools\n",
    "import sys\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "from datetime import datetime\n",
    "import random\n",
    "from IPython.display import Image\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, Dense, BatchNormalization, Dropout, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load TensorBoard Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data\n",
    "\n",
    "This section will cover the data loading and preprocessing steps. \n",
    "\n",
    "The step by step process is as follows:\n",
    "1. [Mounting Google Drive](#step1) - This step is necessary to access the dataset stored in Google Drive.\n",
    "   \n",
    "2. [Extracting the Dataset](#step2) - The dataset is stored in a zip file. We will extract the contents of the zip file to access the dataset.\n",
    "   \n",
    "3. [Copying the Data to the Local Directory](#step3) - We will copy the dataset to the local directory to facilitate data loading.\n",
    "   \n",
    "4. [Defining Directories and Parameters](#step4) - We will define the directories and parameters required for data loading.\n",
    "   \n",
    "5. [Checking Column Names](#step5) - We will check the column names to ensure that they are clean and consistent.\n",
    "   \n",
    "6. [Cleaning Column Names](#step6) - We will clean the column names to ensure that they are consistent and easy to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "\n",
    "### 2.1 Mounting Google Drive\n",
    "\n",
    "The code mounts Google Drive to access the dataset stored there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "\n",
    "### 2.2 Extracting the Dataset and Create Train & Validation Directory\n",
    "\n",
    "The code defines the path to the dataset zip file and the path where the dataset will be extracted. It then checks if the data is already extracted. If not, it extracts the zip file. additioanlly, it prints the number of images for each directory, and create the train and validation directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths\n",
    "ZIP_FILE_PATH = \"/content/drive/Shareddrives/Capstone_Project/Machine_Learning/data/skin_problem_dataset.zip\"\n",
    "DESTINATION_PATH = \"/content/drive/Shareddrives/Capstone_Project/Machine_Learning/data/\"\n",
    "\n",
    "# Define the skin classes\n",
    "SKIN_CLASSES = ['acnes', 'blackheads', 'darkspots', 'wrinkles']\n",
    "\n",
    "# Define root directory\n",
    "ROOT_DIR = '/content/drive/My Drive/Capstone_Project_Bangkit/Machine_Learning/data/split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataset(zip_file_path, destination_path):\n",
    "    # Check if the destination directory already exists\n",
    "    if not os.path.exists(destination_path):\n",
    "        # Create a ZipFile object\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            # Extract all the contents of the zip file to the destination directory\n",
    "            zip_ref.extractall(destination_path)\n",
    "            print(f\"Zip file extracted to {destination_path}\")\n",
    "    else:\n",
    "        print(f\"Directory {destination_path} already exists, skipping extraction\\n\")\n",
    "\n",
    "    # Add print statement to check the content of the data directory\n",
    "    print(f\"Content of {destination_path}: {os.listdir(destination_path)}\\n\")\n",
    "\n",
    "def print_num_images(source_dir, skin_classes):\n",
    "    # For each skin class, print the number of images\n",
    "    for skin_class in skin_classes:\n",
    "        class_dir = os.path.join(source_dir, skin_class)\n",
    "        try:\n",
    "            num_images = len(os.listdir(class_dir))\n",
    "            print(f\"The number of images in {skin_class} class: {num_images}\\n\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Directory {class_dir} does not exist.\")\n",
    "        except NotADirectoryError:\n",
    "            print(f\"{class_dir} is not a directory.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "def create_train_val_dirs(root_path, skin_classes):\n",
    "    # Empty directory to prevent FileExistsError if the function is run several times\n",
    "    if os.path.exists(root_path):\n",
    "        shutil.rmtree(root_path)\n",
    "\n",
    "    # Create train and validation directories\n",
    "    # train and validation directories for skin-case\n",
    "    train_dir = os.path.join(root_path, 'training')\n",
    "    val_dir = os.path.join(root_path, 'validation')\n",
    "\n",
    "    # Create directories for each skin class\n",
    "    for skin_class in skin_classes:\n",
    "        # Create train directory for this skin class\n",
    "        train_class_dir = os.path.join(train_dir, skin_class)\n",
    "        os.makedirs(train_class_dir, exist_ok=True)\n",
    "\n",
    "        # Create validation directory for this skin class\n",
    "        val_class_dir = os.path.join(val_dir, skin_class)\n",
    "        os.makedirs(val_class_dir, exist_ok=True)\n",
    "\n",
    "    # Show all the directories in the root directory\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        for subdir in dirs:\n",
    "            print(os.path.join(root, subdir))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    extract_dataset(ZIP_FILE_PATH, DESTINATION_PATH)\n",
    "    print_num_images(DESTINATION_PATH, SKIN_CLASSES)\n",
    "    create_train_val_dirs(ROOT_DIR, SKIN_CLASSES)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "\n",
    "### 2.3 Split the dataset into train and validation\n",
    "\n",
    "This section will cover the process of splitting the dataset into training and validation sets. The training set will be used to train the model, while the validation set will be used to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(SOURCE_DIR, TRAINING_DIR, VALIDATION_DIR, SPLIT_SIZE):\n",
    "    \"\"\"\n",
    "    Splits the data in the source directory into a training set and a validation set.\n",
    "\n",
    "    Parameters:\n",
    "    SOURCE_DIR (str): The source directory.\n",
    "    TRAINING_DIR (str): The directory where the training set should be copied.\n",
    "    VALIDATION_DIR (str): The directory where the validation set should be copied.\n",
    "    SPLIT_SIZE (float): The proportion of the data to be used for the training set.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    files = [filename for filename in os.listdir(SOURCE_DIR) if os.path.getsize(os.path.join(SOURCE_DIR, filename)) > 0]\n",
    "\n",
    "    train_length = int(len(files) * SPLIT_SIZE)\n",
    "    shuffled = random.sample(files, len(files))\n",
    "    train_set = shuffled[:train_length]\n",
    "    test_set = shuffled[train_length:]\n",
    "\n",
    "    for filename in train_set:\n",
    "        copy_file(SOURCE_DIR, TRAINING_DIR, filename)\n",
    "\n",
    "    for filename in test_set:\n",
    "        copy_file(SOURCE_DIR, VALIDATION_DIR, filename)\n",
    "\n",
    "def copy_file(SOURCE_DIR, DEST_DIR, filename):\n",
    "    src_file = os.path.join(SOURCE_DIR, filename)\n",
    "    dest_file = os.path.join(DEST_DIR, filename)\n",
    "    try:\n",
    "        copyfile(src_file, dest_file)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while copying {src_file} to {dest_file}: {e}\")\n",
    "\n",
    "def clear_directory(DIR):\n",
    "    if len(os.listdir(DIR)) > 0:\n",
    "        for file in os.scandir(DIR):\n",
    "            os.remove(file.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "BASE_DIR = \"/content/drive/My Drive/Capstone_Project_Bangkit/Machine_Learning/data/skin_problem_dataset\"\n",
    "TRAINING_DIR = \"/content/drive/My Drive/Capstone_Project_Bangkit/Machine_Learning/data/split/training\"\n",
    "VALIDATION_DIR = \"/content/drive/My Drive/Capstone_Project_Bangkit/Machine_Learning/data/split/validation\"\n",
    "\n",
    "# Define skin problems\n",
    "SKIN_PROBLEMS = [\"acnes\", \"blackheads\", \"darkspots\", \"wrinkles\"]\n",
    "\n",
    "# Define proportion of images used for training\n",
    "split_size = .8\n",
    "\n",
    "# Split data for each skin problem\n",
    "for problem in SKIN_PROBLEMS:\n",
    "    SOURCE_DIR = os.path.join(BASE_DIR, problem)\n",
    "    TRAINING_PROBLEM_DIR = os.path.join(TRAINING_DIR, problem)\n",
    "    VALIDATION_PROBLEM_DIR = os.path.join(VALIDATION_DIR, problem)\n",
    "\n",
    "    clear_directory(TRAINING_PROBLEM_DIR)\n",
    "    clear_directory(VALIDATION_PROBLEM_DIR)\n",
    "\n",
    "    split_data(SOURCE_DIR, TRAINING_PROBLEM_DIR, VALIDATION_PROBLEM_DIR, split_size)\n",
    "\n",
    "    print(f\"\\n\\nOriginal {problem}'s directory has {len(os.listdir(SOURCE_DIR))} images\")\n",
    "    print(f\"\\n\\nThere are {len(os.listdir(TRAINING_PROBLEM_DIR))} images of {problem} for training\")\n",
    "    print(f\"\\n\\nThere are {len(os.listdir(VALIDATION_PROBLEM_DIR))} images of {problem} for validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Augmentation\n",
    "\n",
    "This section will cover the data augmentation process.\n",
    "\n",
    "The step by step process is as follows:\n",
    "\n",
    "1. [Creating Data Generators](#step7) - We will create data generators for the training, validation, and test sets.\n",
    "\n",
    "2. [Visualizing Images](#step8) - We will visualize some images from the training set to understand the data better.\n",
    "   \n",
    "3. [Checking Batch Sizes](#step9) - We will check the batch sizes of the data generators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step4'></a>\n",
    "\n",
    "### 3.1 Creating Data Generators\n",
    "\n",
    "This section will cover the creation of the image data generator. The image data generator is used to load the images from the dataset and preprocess them before feeding them into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "CLASS_MODE = 'categorical'\n",
    "RESCALE = 1./255\n",
    "\n",
    "# Define data augmentation parameters\n",
    "data_augmentation_params = {\n",
    "    \"rescale\": RESCALE,\n",
    "    \"rotation_range\": 40,\n",
    "    \"width_shift_range\": 0.2,\n",
    "    \"height_shift_range\": 0.2,\n",
    "    \"shear_range\": 0.2,\n",
    "    \"zoom_range\": 0.2,\n",
    "    \"horizontal_flip\": True,\n",
    "    \"fill_mode\": 'nearest'\n",
    "}\n",
    "\n",
    "# Define generator parameters\n",
    "generator_params = {\n",
    "    \"directory\": TRAINING_DIR,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"class_mode\": CLASS_MODE,\n",
    "    \"target_size\": IMAGE_SIZE\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the ImageDataGenerator class for training data with augmentation\n",
    "train_datagen = ImageDataGenerator(**data_augmentation_params)\n",
    "\n",
    "# Pass in the appropriate arguments to the flow_from_directory method\n",
    "train_generator = train_datagen.flow_from_directory(**generator_params)\n",
    "\n",
    "# Instantiate the ImageDataGenerator class for validation data without augmentation\n",
    "validation_datagen = ImageDataGenerator(rescale=RESCALE)\n",
    "\n",
    "# Pass in the appropriate arguments to the flow_from_directory method\n",
    "validation_generator = validation_datagen.flow_from_directory(**generator_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single batch of images and labels from each generator\n",
    "train_images, train_labels = next(train_generator)\n",
    "validation_images, validation_labels = next(validation_generator)\n",
    "\n",
    "# Print the shape of the images and labels\n",
    "print(f\"Image batch of training generator has shape: {train_images.shape}\")\n",
    "print(f\"Image batch of validation generator has shape: {validation_images.shape}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Label batch of training generator has shape: {train_labels.shape}\")\n",
    "print(f\"Label batch of validation generator has shape: {validation_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step5'></a>\n",
    "\n",
    "### 3.2 Visualizing Images\n",
    "\n",
    "This section will cover the visualization of images from the training set. The images will be displayed along with their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_images(dataset, num_images):\n",
    "    \"\"\"\n",
    "    This function takes a dataset and a number of images to display. It selects a random sample of images from the dataset\n",
    "    and displays them in a grid.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (DataFrameIterator): The dataset to select images from. This should be a TensorFlow DataFrameIterator object.\n",
    "    num_images (int): The number of images to display. This should be a positive integer.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Define the labels list\n",
    "    labels_list = ['Acnes', 'Blackheads', 'Darkspots', 'Wrinkles']\n",
    "\n",
    "    # Take one batch from the dataset\n",
    "    images, labels = next(dataset)\n",
    "\n",
    "    # Select a few random images from the batch\n",
    "    random_indices = random.sample(range(images.shape[0]), num_images)\n",
    "    selected_images = images[random_indices]\n",
    "    selected_labels = labels[random_indices]\n",
    "\n",
    "    # Map the one-hot encoded labels back to their original string labels\n",
    "    selected_labels = [labels_list[np.argmax(label)] for label in selected_labels]\n",
    "\n",
    "    # Display the selected images and their labels\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(num_images):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(selected_images[i])\n",
    "        plt.title(f\"Label: {selected_labels[i]}\")\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize images from each generator\n",
    "visualize_images(train_generator, 9)\n",
    "visualize_images(validation_generator, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step5'></a>\n",
    "\n",
    "### 3.3 Checking Batch Sizes\n",
    "\n",
    "This section will cover the checking of batch sizes for the data generators. The batch size is the number of images that will be fed into the model at once during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_batch_size(dataset):\n",
    "    \"\"\"\n",
    "    This function takes a DataFrameIterator dataset and prints out the size of a batch.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (DataFrameIterator): The dataset to check the batch size of. This should be a TensorFlow DataFrameIterator object.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Take one batch from the dataset\n",
    "    images, _ = next(dataset)\n",
    "\n",
    "    # Return the batch size\n",
    "    return images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the batch size of the training, validation, and test datasets\n",
    "print(f\"Batch size of the training dataset: {check_batch_size(train_generator)}\")\n",
    "print(f\"Batch size of the validation dataset: {check_batch_size(validation_generator)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Model\n",
    "\n",
    "This section will cover the model building process.\n",
    "\n",
    "The step by step process is as follows:\n",
    "- Define the model architecture\n",
    "- Compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Define the Model Architecture for Tune Learning Rate\n",
    "\n",
    "This section will cover the definition of the model architecture. The model architecture defines the structure of the neural network, including the number of layers, the type of layers, and the activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Constants for the image dimensions\n",
    "# IMG_HEIGHT = 224\n",
    "# IMG_WIDTH = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_uncompiled_model():\n",
    "\n",
    "#     model = tf.keras.models.Sequential(\n",
    "#         [\n",
    "#           # First Convolutional Block\n",
    "#           Conv2D(16, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "#           MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "#           # Second Convolutional Block\n",
    "#           Conv2D(32, (3, 3), activation='relu'),\n",
    "#           MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "#           # Third Convolutional Block\n",
    "#           Conv2D(64, (3, 3), activation='relu'),\n",
    "#           MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "#           # Flatten the output and add Dense layers\n",
    "#           Flatten(),\n",
    "\n",
    "#           # Add Dense layer\n",
    "#           Dense(512, activation='relu'),\n",
    "#           Dense(64, activation='relu'),\n",
    "#           Dropout(0.5),\n",
    "\n",
    "#           # Output layer\n",
    "#           Dense(4, activation='sigmoid') # 4 skin problems\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def adjust_learning_rate(dataset):\n",
    "\n",
    "#     model = create_uncompiled_model()\n",
    "\n",
    "#     lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-5 * 10**(epoch / 20))\n",
    "\n",
    "#     # Select your optimizer\n",
    "#     optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "#     # Compile the model passing in the appropriate loss\n",
    "#     model.compile(loss='binary_crossentropy',\n",
    "#                   optimizer=optimizer,\n",
    "#                   metrics=[\"accuracy\"])\n",
    "\n",
    "#     history = model.fit(\n",
    "#         train_generator,\n",
    "#         validation_data=validation_generator,\n",
    "#         epochs=100,\n",
    "#         callbacks=[lr_schedule]\n",
    "#     )\n",
    "\n",
    "#     return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training with dynamic LR\n",
    "# lr_history = adjust_learning_rate(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.semilogx(lr_history.history[\"lr\"], lr_history.history[\"loss\"])\n",
    "# plt.axis([1e-6, 1e-1, 0, 0.8])\n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Define the Model Architecture after Tuning Learning Rate\n",
    "\n",
    "This section will cover the definition of the model architecture. The model architecture defines the structure of the neural network, including the number of layers, the type of layers, and the activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for the image dimensions and number of classes\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "NUM_CLASSES = 4  # Number of skin problems\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "    Creates a Sequential model with three convolutional blocks, a flatten layer,\n",
    "    two dense layers, and an output layer.\n",
    "\n",
    "    Returns:\n",
    "        model: A Sequential model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # First Convolutional Block\n",
    "        Conv2D(16, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        # Second Convolutional Block\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        # Third Convolutional Block\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        # Flatten the output of the convolutions\n",
    "        Flatten(),\n",
    "\n",
    "        # Add Dense layers\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        # Output layer\n",
    "        Dense(NUM_CLASSES, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def compile_model(model):\n",
    "    \"\"\"\n",
    "    Compiles the model with binary crossentropy as the loss function,\n",
    "    Adam as the optimizer, and accuracy as the metric.\n",
    "\n",
    "    Args:\n",
    "        model: The model to compile.\n",
    "    \"\"\"\n",
    "    optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "def print_and_plot_model(model):\n",
    "    \"\"\"\n",
    "    Prints the summary of the model and plots the model architecture.\n",
    "\n",
    "    Args:\n",
    "        model: The model to print and plot.\n",
    "    \"\"\"\n",
    "    # Print the model summary\n",
    "    print(f\"Model Summary:\\n{model.summary()}\")\n",
    "\n",
    "    # Plot the model architecture and save it to a file\n",
    "    plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    # Display the model architecture image\n",
    "    display(Image(filename='model.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, compile, print and plot the model\n",
    "model = create_model()\n",
    "compile_model(model)\n",
    "print_and_plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model\n",
    "\n",
    "This section will cover the model training process.\n",
    "\n",
    "The step by step process is as follows:\n",
    "- Fit the model on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Define the constants for the model training\n",
    "\n",
    "This section will cover the definition of the constants required for the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for the confusion matrix plot\n",
    "FIGURE_SIZE = (8, 8)  # Size of the figure\n",
    "CMAP = plt.cm.Blues  # Color map to use\n",
    "TITLE = \"Confusion Matrix\"  # Title of the plot\n",
    "ROTATION_ANGLE = 45  # Angle to rotate the x-axis labels\n",
    "LABELS = ['True label', 'Predicted label']  # Labels for the axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Define the utility function for training the model\n",
    "\n",
    "This section will cover the definition of the utility function for training the model including plot confusion matrix, convert the matplotlib figure to a png file, calculate confusion matrix, and train the model with the TensorBoard callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        cm (np.array): The confusion matrix to plot.\n",
    "        class_names (list): The names of the classes.\n",
    "\n",
    "    Returns:\n",
    "        figure: A matplotlib figure containing the plotted confusion matrix.\n",
    "    \"\"\"\n",
    "    # Create a new figure\n",
    "    figure = plt.figure(figsize=FIGURE_SIZE)\n",
    "\n",
    "    # Display the confusion matrix as an image\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=CMAP)\n",
    "\n",
    "    # Add a title and a color bar\n",
    "    plt.title(TITLE)\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Add ticks and labels\n",
    "    plot_ticks(class_names)\n",
    "\n",
    "    # Normalize the confusion matrix\n",
    "    cm = normalize_confusion_matrix(cm)\n",
    "\n",
    "    # Add the values of the confusion matrix to the plot\n",
    "    plot_values_on_matrix(cm)\n",
    "\n",
    "    # Adjust the layout and add labels\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(LABELS[0])\n",
    "    plt.xlabel(LABELS[1])\n",
    "\n",
    "    return figure\n",
    "\n",
    "def plot_ticks(class_names):\n",
    "    \"\"\"\n",
    "    Adds ticks and labels to the plot.\n",
    "\n",
    "    Args:\n",
    "        class_names (list): The names of the classes.\n",
    "    \"\"\"\n",
    "    # Calculate the positions of the ticks\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "\n",
    "    # Add the ticks and labels to the plot\n",
    "    plt.xticks(tick_marks, class_names, rotation=ROTATION_ANGLE)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "def normalize_confusion_matrix(cm):\n",
    "    \"\"\"\n",
    "    Normalizes a confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        cm (np.array): The confusion matrix to normalize.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The normalized confusion matrix.\n",
    "    \"\"\"\n",
    "    # Normalize the confusion matrix by row (i.e., by the number of samples in each class)\n",
    "    return np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "def plot_values_on_matrix(cm):\n",
    "    \"\"\"\n",
    "    Adds the values of the confusion matrix to the plot.\n",
    "\n",
    "    Args:\n",
    "        cm (np.array): The confusion matrix.\n",
    "    \"\"\"\n",
    "    # Calculate the threshold for the text color\n",
    "    threshold = cm.max() / 2.\n",
    "\n",
    "    # Add the values of the confusion matrix to the plot\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_to_image(figure):\n",
    "    \"\"\"\n",
    "    Converts the matplotlib plot specified by 'figure' to a PNG image and returns it.\n",
    "    The image is returned as a 4D tensor, which can be used as an image summary in TensorBoard.\n",
    "\n",
    "    Args:\n",
    "        figure (matplotlib.figure.Figure): a matplotlib figure object\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: a 4D tensor representing the PNG image\n",
    "    \"\"\"\n",
    "    # Create a BytesIO object and save the matplotlib figure as a PNG in it\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    \n",
    "    # Closing the figure prevents it from being displayed directly inside the notebook\n",
    "    plt.close(figure)\n",
    "    \n",
    "    # Use tf.image.decode_png to convert the PNG buffer to a TF image\n",
    "    buf.seek(0)\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    \n",
    "    # Add a batch dimension and return the image\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_confusion_matrix(epoch, logs, model, validation_generator, file_writer_cm):\n",
    "    \"\"\"\n",
    "    Logs the confusion matrix as an image summary in TensorBoard.\n",
    "\n",
    "    Args:\n",
    "        epoch (int): The current epoch number.\n",
    "        logs (dict): A dictionary of logs.\n",
    "        model (tf.keras.Model): The model being trained.\n",
    "        validation_generator (tf.keras.preprocessing.image.DirectoryIterator): The validation data generator.\n",
    "        file_writer_cm (tf.summary.FileWriter): The FileWriter for the confusion matrix.\n",
    "    \"\"\"\n",
    "    # Predict the probabilities using validation generator\n",
    "    predictions_prob = model.predict(validation_generator)\n",
    "    \n",
    "    # Convert probabilities to class labels\n",
    "    predictions = np.argmax(predictions_prob, axis=1)\n",
    "\n",
    "    # Get the true class labels\n",
    "    true_labels = validation_generator.classes\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Create a figure with the confusion matrix\n",
    "    figure = plot_confusion_matrix(cm, class_names=validation_generator.class_indices)\n",
    "    \n",
    "    # Convert the figure to a TensorBoard-compatible image\n",
    "    cm_image = plot_to_image(figure)\n",
    "\n",
    "    # Log the confusion matrix as an image summary\n",
    "    with file_writer_cm.as_default():\n",
    "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_generator, validation_generator):\n",
    "    \"\"\"\n",
    "    Trains the model and logs the training process to TensorBoard.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The model to be trained.\n",
    "        train_generator (tf.keras.preprocessing.image.DirectoryIterator): The training data generator.\n",
    "        validation_generator (tf.keras.preprocessing.image.DirectoryIterator): The validation data generator.\n",
    "    \"\"\"\n",
    "    # Clear logs prior to logging data\n",
    "    os.system('rm -rf ./logs/')\n",
    "\n",
    "    # Define the log directory\n",
    "    logdir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    # Create a TensorBoard callback\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)\n",
    "\n",
    "    # Create a file writer for writing custom metrics to TensorBoard\n",
    "    file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')\n",
    "\n",
    "    # Define the per-epoch callback. This will log the confusion matrix at the end of each epoch\n",
    "    cm_callback = tf.keras.callbacks.LambdaCallback(\n",
    "        on_epoch_end=lambda epoch, logs: log_confusion_matrix(epoch, logs, model, validation_generator, file_writer_cm)\n",
    "    )\n",
    "\n",
    "    # Train the classifier\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=100,\n",
    "        verbose=0,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[tensorboard_callback, cm_callback]\n",
    "    )\n",
    "    # history object contains details about the training process\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Train the model\n",
    "\n",
    "This section will cover the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start TensorBoard\n",
    "%tensorboard --logdir logs/fit --host localhost\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_generator, validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model\n",
    "\n",
    "This section will cover the model evaluation process.\n",
    "\n",
    "The step by step process is as follows:\n",
    "- Evaluate the model on the test data\n",
    "- Generate predictions\n",
    "- Print classification report\n",
    "- Plot confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Model Evaluation\n",
    "\n",
    "This section is responsible for evaluatin the trained model and saving the results. The evaluation process includes generating predictions, printing the classification report, and plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create a text file named \"model_summary.txt\" and write the model summary to it\n",
    "    with open('model_summary.txt', 'w') as f:\n",
    "        sys.stdout = f  # Change the standard output to the file we created.\n",
    "        model.summary()  # This will print the summary to the file\n",
    "        sys.stdout = sys.__stdout__  # Reset the standard output to its original value.\n",
    "\n",
    "    # Generate predictions\n",
    "    y_pred = model.predict(validation_generator)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Print the classification report\n",
    "    print(classification_report(validation_generator.classes, y_pred_classes))\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(validation_generator.classes, y_pred_classes)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "\n",
    "    # Define the evaluation results\n",
    "    evaluation_results = model.evaluate(validation_generator, verbose=0)\n",
    "\n",
    "    # Convert the evaluation results to a pandas DataFrame\n",
    "    evaluation_df = pd.DataFrame([evaluation_results], columns=['Loss', 'Accuracy'])\n",
    "\n",
    "    # Create a CSV file named \"evaluation_results.csv\" and write the evaluation results to it\n",
    "    evaluation_df.to_csv('evaluation_results.csv', index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Model\n",
    "\n",
    "This section will cover the model saving process.\n",
    "\n",
    "The step by step process is as follows:\n",
    "- Save the model for future use\n",
    "- Save the model architecture\n",
    "- Save the model weights\n",
    "- Save the model history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Convert Model to TensorFlow Lite\n",
    "\n",
    "This section will cover the model conversion process.\n",
    "\n",
    "The step by step process is as follows:\n",
    "- Convert the model to the TensorFlow Lite format (.tflite) with quantization to reduce the model size\n",
    "- Save the converted model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Integerate with Mobile Device (Android)\n",
    "\n",
    "This section will cover the integration of the model with an Android application.\n",
    "\n",
    "The step by step process is as follows:\n",
    "- Integrate with an Android application developed by team Mobile Development\n",
    "- Load the model in the Android application\n",
    "- Perform inference on the device\n",
    "- Display the results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
