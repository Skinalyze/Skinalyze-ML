{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skin Problem Classification Notebook \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Information\n",
    "\n",
    "**Team ID:** C241-PS385  \n",
    "\n",
    "**Members:**    \n",
    "- Stefanus Bernard Melkisedek - [GitHub Profile](https://github.com/stefansphtr)\n",
    "- Debby Trinita - [GitHub Profile](https://github.com/debbytrinita)\n",
    "- Mhd. Reza Kurniawan Lubis - [GitHub Profile](https://github.com/rezakur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen Development Environment\n",
    "\n",
    "For this project, our team opted to utilize Google Colab as our primary development environment. The decision to use Google Colab was primarily driven by its provision of complimentary access to GPU and TPU resources. These resources significantly expedite the model training process, thereby enhancing our productivity and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data\n",
    "\n",
    "This section will cover the data loading and preprocessing steps. \n",
    "\n",
    "The step by step process is as follows:\n",
    "1. **Mounting Google Drive** - This step is necessary to access the dataset stored in Google Drive.\n",
    "   \n",
    "2. **Extracting the Dataset** - The dataset is stored in a zip file. We will extract the contents of the zip file to access the dataset.\n",
    "   \n",
    "3. **Copying the Data to the Local Directory** - We will copy the dataset to the local directory to facilitate data loading.\n",
    "   \n",
    "4. **Defining Directories and Parameters** - We will define the directories and parameters required for data loading.\n",
    "   \n",
    "5. **Checking Column Names** - We will check the column names to ensure that they are clean and consistent.\n",
    "   \n",
    "6. **Cleaning Column Names** - We will clean the column names to ensure that they are consistent and easy to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Mounting Google Drive\n",
    "\n",
    "The code mounts Google Drive to access the dataset stored there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Extracting the Dataset\n",
    "\n",
    "The code defines the path to the dataset zip file and the path where the dataset will be extracted. It then checks if the data is already extracted. If not, it extracts the zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the dataset zip file\n",
    "dataset_zip_file_path = '/content/drive/Shareddrives/Capstone_Project/Machine_Learning/data/skin_problem_dataset.zip'\n",
    "\n",
    "# Define the path where the dataset will be extracted\n",
    "extraction_path = '/content/drive/Shareddrives/Capstone_Project/Machine_Learning/data/'\n",
    "\n",
    "# Check if the data is already extracted\n",
    "if not os.path.exists(extraction_path):\n",
    "    # Open the dataset zip file in read mode\n",
    "    with zipfile.ZipFile(dataset_zip_file_path, 'r') as dataset_zip_file:\n",
    "        try:\n",
    "            # Extract all files from the dataset zip file to the defined path\n",
    "            dataset_zip_file.extractall(extraction_path)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while extracting the zip file: {e}\")\n",
    "else:\n",
    "    print(\"Data is already extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Copying the Data to the Local Directory\n",
    "\n",
    "The code defines the source and destination directories and copies the data from the source to the destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define source and destination directories\n",
    "source_dir = '/content/drive/Shareddrives/Capstone_Project/Machine_Learning/data/'\n",
    "destination_dir = '/content/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the data to the local environment\n",
    "try:\n",
    "    if not os.path.exists(destination_dir):\n",
    "        shutil.copytree(source_dir, destination_dir)\n",
    "    else:\n",
    "        print(\"Destination directory already exists. Files were not copied.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while copying files: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
