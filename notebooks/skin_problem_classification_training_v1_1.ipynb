{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skin Problem Classification Notebook \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Information\n",
    "\n",
    "**Team ID:** C241-PS385  \n",
    "\n",
    "**Members:**    \n",
    "- Stefanus Bernard Melkisedek - [GitHub Profile](https://github.com/stefansphtr)\n",
    "- Debby Trinita - [GitHub Profile](https://github.com/debbytrinita)\n",
    "- Mhd. Reza Kurniawan Lubis - [GitHub Profile](https://github.com/rezakur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen Development Environment\n",
    "\n",
    "For this project, our team opted to utilize Google Colab as our primary development environment. The decision to use Google Colab was primarily driven by its provision of complimentary access to GPU and TPU resources. These resources significantly expedite the model training process, thereby enhancing our productivity and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data\n",
    "\n",
    "This section will cover the data loading and preprocessing steps. \n",
    "\n",
    "The step by step process is as follows:\n",
    "1. [Mounting Google Drive](#step1) - This step is necessary to access the dataset stored in Google Drive.\n",
    "   \n",
    "2. [Extracting the Dataset](#step2) - The dataset is stored in a zip file. We will extract the contents of the zip file to access the dataset.\n",
    "   \n",
    "3. [Copying the Data to the Local Directory](#step3) - We will copy the dataset to the local directory to facilitate data loading.\n",
    "   \n",
    "4. [Defining Directories and Parameters](#step4) - We will define the directories and parameters required for data loading.\n",
    "   \n",
    "5. [Checking Column Names](#step5) - We will check the column names to ensure that they are clean and consistent.\n",
    "   \n",
    "6. [Cleaning Column Names](#step6) - We will clean the column names to ensure that they are consistent and easy to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "\n",
    "### 2.1 Mounting Google Drive\n",
    "\n",
    "The code mounts Google Drive to access the dataset stored there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "\n",
    "### 2.2 Extracting the Dataset\n",
    "\n",
    "The code defines the path to the dataset zip file and the path where the dataset will be extracted. It then checks if the data is already extracted. If not, it extracts the zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the dataset zip file\n",
    "dataset_zip_file_path = '/content/drive/Shareddrives/Capstone_Project/Machine_Learning/data/skin_problem_dataset.zip'\n",
    "\n",
    "# Define the path where the dataset will be extracted\n",
    "extraction_path = '/content/drive/Shareddrives/Capstone_Project/Machine_Learning/data/'\n",
    "\n",
    "# Check if the data is already extracted\n",
    "if not os.path.exists(extraction_path):\n",
    "    # Open the dataset zip file in read mode\n",
    "    with zipfile.ZipFile(dataset_zip_file_path, 'r') as dataset_zip_file:\n",
    "        try:\n",
    "            # Extract all files from the dataset zip file to the defined path\n",
    "            dataset_zip_file.extractall(extraction_path)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while extracting the zip file: {e}\")\n",
    "else:\n",
    "    print(\"Data is already extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "\n",
    "### 2.3 Copying the Data to the Local Directory\n",
    "\n",
    "The code defines the source and destination directories and copies the data from the source to the destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define source and destination directories\n",
    "source_dir = '/content/drive/Shareddrives/Capstone_Project/Machine_Learning/data/'\n",
    "destination_dir = '/content/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the data to the local environment\n",
    "try:\n",
    "    if not os.path.exists(destination_dir):\n",
    "        shutil.copytree(source_dir, destination_dir)\n",
    "    else:\n",
    "        print(\"Destination directory already exists. Files were not copied.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while copying files: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step4'></a>\n",
    "\n",
    "### 2.4 Defining Directories and Parameters\n",
    "\n",
    "The code defines the directories for the training, validation, and test sets. It also defines the batch size and image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directories\n",
    "train_dir = os.path.join(destination_dir, 'train')\n",
    "val_dir = os.path.join(destination_dir, 'valid')\n",
    "test_dir = os.path.join(destination_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size and image size\n",
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT, IMG_WIDTH = 224, 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step5'></a>\n",
    "\n",
    "### 2.5 Checking Column Names\n",
    "\n",
    "The code loads the CSV files from each directory into pandas DataFrames and prints the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the columns name in each directory\n",
    "df_train = pd.read_csv(os.path.join(train_dir, '_classes.csv'))\n",
    "df_validation = pd.read_csv(os.path.join(val_dir, '_classes.csv'))\n",
    "df_test = pd.read_csv(os.path.join(test_dir, '_classes.csv'))\n",
    "\n",
    "# Print the columns name in each directory\n",
    "print(f\"Columns in the training directory: {df_train.columns}\\n\")\n",
    "print(f\"Columns in the validation directory: {df_validation.columns}\\n\")\n",
    "print(f\"Columns in the test directory: {df_test.columns}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It seems there are some issues with the column names in the CSV files. We will remove the extra spaces from the column names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step6'></a>\n",
    "\n",
    "### 2.6 Cleaning Column Names\n",
    "\n",
    "The code removes the trailing whitespace from the column names and checks the cleaned column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the trailing whitespace from the column names\n",
    "df_train.columns = df_train.columns.str.strip()\n",
    "df_validation.columns = df_validation.columns.str.strip()\n",
    "df_test.columns = df_test.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the trailing whitespace in the column names\n",
    "print(f\"Columns in the training directory: {df_train.columns}\\n\")\n",
    "print(f\"Columns in the validation directory: {df_validation.columns}\\n\")\n",
    "print(f\"Columns in the test directory: {df_test.columns}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Augmentation\n",
    "\n",
    "This section will cover the data augmentation process.\n",
    "\n",
    "The step by step process is as follows:\n",
    "\n",
    "1. [Creating Data Generators](#step7) - We will create data generators for the training, validation, and test sets.\n",
    "\n",
    "2. [Visualizing Images](#step8) - We will visualize some images from the training set to understand the data better.\n",
    "   \n",
    "3. [Checking Labels](#step9) - We will check the distribution of labels in the training, validation, and test sets.\n",
    "\n",
    "4. [Checking Dataset Sizes](#step10) - We will check the sizes of the training, validation, and test sets.\n",
    "\n",
    "5. [Checking Batch Sizes](#step11) - We will check the batch sizes of the data generators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step7'></a>\n",
    "\n",
    "### 3.1 Creating Data Generators\n",
    "\n",
    "The code defines a function to create ImageDataGenerators for the training, validation, and test sets. It then calls this function to create the generators and generate a batch of data from each generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_generators(train_dir, val_dir, test_dir, img_height, img_width, batch_size):\n",
    "    \"\"\"\n",
    "    This function creates ImageDataGenerators for the training, validation, and test sets.\n",
    "    It also loads the datasets from CSV files.\n",
    "\n",
    "    Parameters:\n",
    "    train_dir (str): The directory where the training set is located.\n",
    "    val_dir (str): The directory where the validation set is located.\n",
    "    test_dir (str): The directory where the test set is located.\n",
    "    img_height (int): The height of the images.\n",
    "    img_width (int): The width of the images.\n",
    "    batch_size (int): The batch size.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the training, validation, and test generators.\n",
    "    \"\"\"\n",
    "    labels = ['Acne', 'Blackheads', 'Dark Spots', 'Dry Skin', 'Eye bags', 'Normal Skin', 'Oily Skin', 'Pores', 'Skin Redness', 'Wrinkles']\n",
    "\n",
    "    # Create an ImageDataGenerator for the training set\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "    )\n",
    "\n",
    "    # Create an ImageDataGenerator for the validation set\n",
    "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Create an ImageDataGenerator for the test set\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Load the training set from the CSV file\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        df_train,\n",
    "        directory=train_dir,\n",
    "        x_col='filename',\n",
    "        y_col=labels,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='raw')\n",
    "\n",
    "    # Load the validation set from the CSV file\n",
    "    validation_generator = validation_datagen.flow_from_dataframe(\n",
    "        df_validation,\n",
    "        directory=val_dir,\n",
    "        x_col='filename',\n",
    "        y_col=labels,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='raw')\n",
    "\n",
    "    # Load the test set from the CSV file\n",
    "    test_generator = test_datagen.flow_from_dataframe(\n",
    "        df_test,\n",
    "        directory=test_dir,\n",
    "        x_col='filename',\n",
    "        y_col=labels,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='raw')\n",
    "\n",
    "    return train_generator, validation_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generator creation\n",
    "train_generator, validation_generator, test_generator = create_data_generators(train_dir, val_dir, test_dir, IMG_HEIGHT, IMG_WIDTH, BATCH_SIZE)\n",
    "\n",
    "# Generate a batch of data from each generator\n",
    "train_images, train_labels = next(train_generator)\n",
    "validation_images, validation_labels = next(validation_generator)\n",
    "test_images, test_labels = next(test_generator)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Image of training generator have shape: {train_images.shape}\")\n",
    "print(f\"Image of validation generator have shape: {validation_images.shape}\")\n",
    "print(f\"Image of test generator have shape: {test_images.shape}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Labels of training generator have shape: {train_labels.shape}\")\n",
    "print(f\"Labels of validation generator have shape: {validation_labels.shape}\")\n",
    "print(f\"Labels of test generator have shape: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step8'></a>\n",
    "\n",
    "### 3.2 Visualizing Images\n",
    "\n",
    "The code defines a function to visualize a random sample of images from a dataset. It then calls this function to visualize images from each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_images(dataset, num_images):\n",
    "    \"\"\"\n",
    "    This function takes a dataset and a number of images to display. It selects a random sample of images from the dataset\n",
    "    and displays them in a grid.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (DataFrameIterator): The dataset to select images from. This should be a TensorFlow DataFrameIterator object.\n",
    "    num_images (int): The number of images to display. This should be a positive integer.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Define the labels list\n",
    "    labels_list = ['Acne', 'Blackheads', 'Dark Spots', 'Dry Skin', 'Eye bags', 'Normal Skin', 'Oily Skin', 'Pores', 'Skin Redness', 'Wrinkles']\n",
    "\n",
    "    # Take one batch from the dataset\n",
    "    images, labels = next(dataset)\n",
    "\n",
    "    # Select a few random images from the batch\n",
    "    random_indices = random.sample(range(images.shape[0]), num_images)\n",
    "    selected_images = images[random_indices]\n",
    "    selected_labels = labels[random_indices]\n",
    "\n",
    "    # Map the one-hot encoded labels back to their original string labels\n",
    "    selected_labels = [labels_list[np.argmax(label)] for label in selected_labels]\n",
    "\n",
    "    # Display the selected images and their labels\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(num_images):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(selected_images[i])\n",
    "        plt.title(f\"Label: {selected_labels[i]}\")\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize images from each dataset\n",
    "visualize_images(train_generator, 9)\n",
    "visualize_images(validation_generator, 9)\n",
    "visualize_images(test_generator, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step9'></a>\n",
    "\n",
    "### 3.3 Checking Labels\n",
    "\n",
    "The code defines a function to print out the first few labels from a dataset. It then calls this function to check labels from each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_labels(dataset, num_labels):\n",
    "    \"\"\"\n",
    "    This function takes a dataset and prints out the first few labels in their original string form.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (DataFrameIterator): The dataset to select labels from. This should be a TensorFlow DataFrameIterator object.\n",
    "    num_labels (int): The number of labels to print. This should be a positive integer.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Define the labels list\n",
    "    labels_list = ['Acne', 'Blackheads', 'Dark Spots', 'Dry Skin', 'Eye bags', 'Normal Skin', 'Oily Skin', 'Pores', 'Skin Redness', 'Wrinkles']\n",
    "\n",
    "    # Take one batch from the dataset\n",
    "    _, labels = next(dataset)\n",
    "\n",
    "    # Select a few labels from the batch\n",
    "    selected_labels = labels[:num_labels]\n",
    "\n",
    "    # Map the one-hot encoded labels back to their original string labels\n",
    "    selected_labels = [labels_list[np.argmax(label)] for label in selected_labels]\n",
    "\n",
    "    # Print out the selected labels\n",
    "    print(f\"Labels: {selected_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check labels from each dataset\n",
    "check_labels(train_generator, 5)\n",
    "check_labels(validation_generator, 5)\n",
    "check_labels(test_generator, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step10'></a>\n",
    "\n",
    "### 3.4 Checking Dataset Sizes\n",
    "\n",
    "The code defines a function to print out the size of a dataset. It then calls this function to check the sizes of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataset_size(dataset):\n",
    "    \"\"\"\n",
    "    This function takes a DataFrameIterator dataset and prints out its size.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (DataFrameIterator): The dataset to check the size of. This should be a TensorFlow DataFrameIterator object.\n",
    "\n",
    "    Returns:\n",
    "    Tuple: Number of batches and total number of images in the dataset\n",
    "    \"\"\"\n",
    "    # Compute the number of batches in the dataset\n",
    "    num_batches = len(dataset)\n",
    "\n",
    "    # Compute the total number of images in the dataset\n",
    "    num_images = dataset.samples\n",
    "\n",
    "    return num_batches, num_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sizes of each dataset\n",
    "\n",
    "# Print out the number of batches and the total number of images in training dataset\n",
    "num_batches, num_images = check_dataset_size(train_generator)\n",
    "print(f\"Number of batches in the training dataset: {num_batches}\")\n",
    "print(f\"Total number of images in the training dataset: {num_images}\\n\")\n",
    "\n",
    "# Print out the number of batches and the total number of images in validation dataset\n",
    "num_batches, num_images = check_dataset_size(validation_generator)\n",
    "print(f\"Number of batches in the validation dataset: {num_batches}\")\n",
    "print(f\"Total number of images in the validation dataset: {num_images}\\n\")\n",
    "\n",
    "# Print out the number of batches and the total number of images in test dataset\n",
    "num_batches, num_images = check_dataset_size(test_generator)\n",
    "print(f\"Number of batches in the test dataset: {num_batches}\")\n",
    "print(f\"Total number of images in the test dataset: {num_images}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step11'></a>\n",
    "\n",
    "### 3.5 Checking Batch Sizes\n",
    "\n",
    "The code defines a function to print out the size of a batch from a dataset. It then calls this function to check the batch sizes of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_batch_size(dataset):\n",
    "    \"\"\"\n",
    "    This function takes a DataFrameIterator dataset and prints out the size of a batch.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (DataFrameIterator): The dataset to check the batch size of. This should be a TensorFlow DataFrameIterator object.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Take one batch from the dataset\n",
    "    images, _ = next(dataset)\n",
    "\n",
    "    # Print out the size of the batch\n",
    "    print(f\"Batch size: {images.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check batch sizes of each dataset\n",
    "check_batch_size(train_generator)\n",
    "check_batch_size(validation_generator)\n",
    "check_batch_size(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Model\n",
    "\n",
    "This section will cover the model building process.\n",
    "\n",
    "The step by step process is as follows:\n",
    "- Define the model architecture\n",
    "- Compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model\n",
    "\n",
    "This section will cover the model training process.\n",
    "\n",
    "The step by step process is as follows:\n",
    "- Fit the model on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model\n",
    "\n",
    "This section will cover the model evaluation process.\n",
    "\n",
    "The step by step process is as follows:\n",
    "- Evaluate the model on the test data\n",
    "- Generate predictions\n",
    "- Print classification report\n",
    "- Plot confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Model\n",
    "\n",
    "This section will cover the model saving process.\n",
    "\n",
    "The step by step process is as follows:\n",
    "- Save the model for future use\n",
    "- Save the model architecture\n",
    "- Save the model weights\n",
    "- Save the model history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Convert Model to TensorFlow Lite\n",
    "\n",
    "This section will cover the model conversion process.\n",
    "\n",
    "The step by step process is as follows:\n",
    "- Convert the model to the TensorFlow Lite format (.tflite) with quantization to reduce the model size\n",
    "- Save the converted model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Integerate with Mobile Device (Android)\n",
    "\n",
    "This section will cover the integration of the model with an Android application.\n",
    "\n",
    "The step by step process is as follows:\n",
    "- Integrate with an Android application developed by team Mobile Development\n",
    "- Load the model in the Android application\n",
    "- Perform inference on the device\n",
    "- Display the results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
